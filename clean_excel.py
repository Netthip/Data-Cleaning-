# clean_excel.py
import os, re, json, glob, sys
import pandas as pd

# ---------- ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏ï‡∏≤‡∏°‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Å‡∏¥‡πä‡∏ü ----------
INPUT_FOLDER  = r"inputs"     # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÑ‡∏ü‡∏•‡πå‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
OUTPUT_FOLDER = r"outputs"    # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
HEADER_JSON   = r"header_dict.json"
TEST_FILE     = r""  # ‡∏ñ‡πâ‡∏≤‡∏à‡∏∞‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡πÉ‡∏™‡πà‡∏û‡∏≤‡∏ò‡πÄ‡∏ï‡πá‡∏°/‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ó‡∏ò‡πå‡πÑ‡∏î‡πâ ‡πÄ‡∏ä‡πà‡∏ô r"inputs\21101_70MasterData.xlsx"

# ‡∏ñ‡πâ‡∏≤ STRICT=True ‡πÅ‡∏•‡πâ‡∏ß‡∏û‡∏ö issues > 0 ‡∏à‡∏∞ "‡πÑ‡∏°‡πà" export ‡πÑ‡∏ü‡∏•‡πå ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏±‡∏ô‡∏á‡∏≤‡∏ô‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
STRICT = False
# ------------------------------------------

os.makedirs(OUTPUT_FOLDER, exist_ok=True)

# ‡πÇ‡∏´‡∏•‡∏î‡∏û‡∏à‡∏ô‡∏≤‡∏ô‡∏∏‡∏Å‡∏£‡∏°‡∏´‡∏±‡∏ß
with open(HEADER_JSON, "r", encoding="utf-8") as f:
    HEADER_MAP = json.load(f)

# ---------- ‡∏ï‡∏±‡∏ß‡∏ä‡πà‡∏ß‡∏¢ ----------
def normalize_header_names(cols, header_map):
    """‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ (new_cols, unknown_list) ‚Äî ‡∏£‡∏µ‡πÄ‡∏ô‡∏°‡∏´‡∏±‡∏ß‡∏ï‡∏≤‡∏° dict + ‡∏à‡∏±‡∏ö case ‡∏ó‡∏µ‡πà‡∏™‡∏∞‡∏Å‡∏î‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢ (‡∏ï‡∏±‡∏î‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á/‡∏ß‡∏á‡πÄ‡∏•‡πá‡∏ö)"""
    new_cols, unknown = [], []
    for c in cols:
        c0 = str(c).strip()
        if c0 in header_map:
            new_cols.append(header_map[c0]); continue
        c_norm = re.sub(r"[()\s]", "", c0)
        matched = None
        for k, v in header_map.items():
            k_norm = re.sub(r"[()\s]", "", str(k))
            if c_norm.lower() == k_norm.lower():
                matched = v; break
        if matched:
            new_cols.append(matched)
        else:
            new_cols.append(c0)
            unknown.append(c0)
    return new_cols, unknown

def to_numeric_safe(s):
    return pd.to_numeric(s, errors="coerce")

# ---------- ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Å‡∏é Validation ‡∏ó‡∏µ‡πà ‚Äú‡∏Å‡∏±‡∏ô‡∏á‡∏≤‡∏ô‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‚Äù ----------
VALIDATION_CFG = {
    "decimal_places": {     # ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏®‡∏ô‡∏¥‡∏¢‡∏°‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î (‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: 4 ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á)
        "FY67_adjusted": 4, "FY68": 4, "FY69": 4, "FY70": 4
    },
    "non_negative": [       # ‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏°‡πà‡∏ï‡∏¥‡∏î‡∏•‡∏ö
        "FY67_adjusted", "FY68", "FY69", "FY70"
    ],
    "not_null": [           # ‡∏´‡πâ‡∏≤‡∏°‡∏ß‡πà‡∏≤‡∏á
        "activity_code", "item_code"
    ]
}

def validate_df(df):
    """‡∏ï‡∏£‡∏ß‡∏à dataframe ‡∏ï‡πà‡∏≠‡∏ä‡∏µ‡∏ï ‚Üí ‡∏Ñ‡∏∑‡∏ô (issues:list[dict], passed:bool)"""
    issues = []
    # 1) ‡∏ó‡∏®‡∏ô‡∏¥‡∏¢‡∏°‡∏ï‡∏≤‡∏°‡∏Å‡∏≥‡∏´‡∏ô‡∏î
    for col, d in VALIDATION_CFG.get("decimal_places", {}).items():
        if col in df.columns:
            ser = to_numeric_safe(df[col])
            bad_idx = df[~ser.isna() & (ser.round(d) != ser)].index.tolist()
            for i in bad_idx:
                issues.append({"row": int(i)+1, "column": col, "rule": f"decimals_{d}", "value": df.at[i, col]})
    # 2) ‡∏´‡πâ‡∏≤‡∏°‡∏ï‡∏¥‡∏î‡∏•‡∏ö
    for col in VALIDATION_CFG.get("non_negative", []):
        if col in df.columns:
            ser = to_numeric_safe(df[col])
            bad_idx = df[ser < 0].index.tolist()
            for i in bad_idx:
                issues.append({"row": int(i)+1, "column": col, "rule": "non_negative", "value": df.at[i, col]})
    # 3) ‡∏´‡πâ‡∏≤‡∏°‡∏ß‡πà‡∏≤‡∏á
    for col in VALIDATION_CFG.get("not_null", []):
        if col in df.columns:
            bad_idx = df[df[col].isna() | (df[col].astype(str).str.strip() == "")].index.tolist()
            for i in bad_idx:
                issues.append({"row": int(i)+1, "column": col, "rule": "not_null", "value": df.at[i, col]})
    return issues, len(issues) == 0

# ---------- ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î ‚Äú‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÑ‡∏ü‡∏•‡πå‚Äù ----------
def process_one_file(path):
    xls = pd.ExcelFile(path)
    clean_sheets = {}
    all_unknown_headers = set()

    # ‡∏£‡∏µ‡πÄ‡∏ô‡∏°‡∏´‡∏±‡∏ß‡∏ó‡∏∏‡∏Å‡∏ä‡∏µ‡∏ï
    for sheet in xls.sheet_names:
        df = xls.parse(sheet, header=0)
        new_cols, unknown = normalize_header_names(df.columns, HEADER_MAP)
        df.columns = new_cols
        clean_sheets[sheet] = df
        all_unknown_headers.update(unknown)

    # ‡∏£‡∏ß‡∏° issues ‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡∏ä‡∏µ‡∏ï
    issues_all = []
    for sheet, df in clean_sheets.items():
        issues, ok = validate_df(df)
        for it in issues:
            it["sheet"] = sheet
        issues_all.extend(issues)

    base = os.path.splitext(os.path.basename(path))[0]
    out_file = os.path.join(OUTPUT_FOLDER, base + "_Clean.xlsx")
    log_file = os.path.join(OUTPUT_FOLDER, base + "_clean_log.csv")

    # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô log ‡πÄ‡∏™‡∏°‡∏≠
    log_df = pd.DataFrame(issues_all) if issues_all else pd.DataFrame(columns=["row","column","rule","value","sheet"])
    log_df.to_csv(log_file, index=False, encoding="utf-8-sig")

    # ‡πÇ‡∏´‡∏°‡∏î‡∏¢‡∏≠‡∏°/‡πÑ‡∏°‡πà‡∏¢‡∏≠‡∏° export ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ issues
    if STRICT and len(issues_all) > 0:
        # ‡∏Å‡∏±‡∏ô ‚Äú‡∏á‡∏≤‡∏ô‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‚Äù ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏¢‡∏≠‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå Clean
        return {
            "file": os.path.basename(path),
            "exported": False,
            "reason": f"STRICT mode: ‡∏û‡∏ö {len(issues_all)} issues",
            "unknown_headers": "|".join(sorted(all_unknown_headers)) if all_unknown_headers else "",
            "issues_count": len(issues_all),
            "out_file": "",
            "log_file": log_file
        }

    # export ‡πÑ‡∏î‡πâ (‡∏ñ‡∏∂‡∏á‡∏à‡∏∞‡∏°‡∏µ issues ‡∏Å‡πá‡∏¢‡∏≠‡∏° ‡πÅ‡∏ï‡πà‡∏°‡∏µ log ‡πÅ‡∏ô‡∏ö)
    with pd.ExcelWriter(out_file, engine="openpyxl") as writer:
        for sheet, df in clean_sheets.items():
            df.to_excel(writer, sheet_name=sheet[:31], index=False)

    return {
        "file": os.path.basename(path),
        "exported": True,
        "reason": "OK",
        "unknown_headers": "|".join(sorted(all_unknown_headers)) if all_unknown_headers else "",
        "issues_count": len(issues_all),
        "out_file": out_file,
        "log_file": log_file
    }

# ---------- main ----------
def main():
    summary = []

    # ‡πÇ‡∏´‡∏°‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (‡∏ï‡∏±‡πâ‡∏á TEST_FILE)
    if TEST_FILE:
        if not os.path.exists(TEST_FILE):
            print(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå: {TEST_FILE}")
            return
        print(f"‚ñ∂ Processing single file: {TEST_FILE}")
        res = process_one_file(TEST_FILE)
        summary.append(res)
        print(res)
    else:
        # ‡πÇ‡∏´‡∏°‡∏î‡∏ó‡∏±‡πâ‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå
        paths = glob.glob(os.path.join(INPUT_FOLDER, "*.xlsx"))
        if not paths:
            print(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå .xlsx ‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå: {INPUT_FOLDER}")
            return
        for p in paths:
            try:
                print(f"‚ñ∂ {os.path.basename(p)} ...")
                res = process_one_file(p)
                summary.append(res)
                print(f"   ‚Üí exported={res['exported']}, issues={res['issues_count']}")
            except Exception as e:
                summary.append({
                    "file": os.path.basename(p),
                    "exported": False,
                    "reason": f"ERROR: {e}",
                    "unknown_headers": "ERROR",
                    "issues_count": -1,
                    "out_file": "",
                    "log_file": ""
                })
                print(f"   ‚ùå Error: {e}")

    # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏™‡∏£‡∏∏‡∏õ‡∏ó‡∏±‡πâ‡∏á‡∏ä‡∏∏‡∏î
    sum_df = pd.DataFrame(summary)
    sum_path = os.path.join(OUTPUT_FOLDER, "batch_summary.csv")
    sum_df.to_csv(sum_path, index=False, encoding="utf-8-sig")
    print(f"\nüßæ Summary ‚Üí {sum_path}")
    print("‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß‡∏à‡πâ‡∏≤ ‚ú®")

if __name__ == "__main__":
    main()
# ‡∏•‡∏¥‡∏™‡∏ï‡πå‡∏´‡∏±‡∏ß‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏≤‡∏Å‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡πÄ‡∏™‡∏°‡∏≠ (Master Schema)
MASTER_SCHEMA = [
    "program",
    "project",
    "activity_code",
    "activity_name",
    "budget_category",
    "expense_nature",
    "expense_group",
    "single_or_multi_year",
    "opex_or_capex",
    "item_code",
    "item_name",
    "item_sub",
    "FY67_adjusted",
    "FY68",
    "FY69",
    "FY70"
]

